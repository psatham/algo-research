{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "This project analyzes daily stock market data and models their behavior using a Fourier Series to uncover and reconstruct underlying cyclical patterns.\n",
    "\n",
    "### Why use a Fourier Series?\n",
    "\n",
    "The premise is that any curve can be described as a sum of many different sine and cosine waves. What if we take a seemingly random curve, like stock market data, and see if we can recreate it with sine and cosine waves, and use that model to forecast future prices?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "\n",
    "Install dependencies quietly, so logs don’t reveal sensitive info like absolute paths or user-specific details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas polygon-api-client matplotlib -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from polygon import RESTClient\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Iterable, Optional, Tuple, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Daily Aggregates\n",
    "\n",
    "Use Polygon.io REST API to get daily aggregated bars. Docs: https://github.com/polygon-io/client-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"<insert API key here>\"\n",
    "client = RESTClient(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_daily_aggregates(symbol, start_date, end_date):\n",
    "    aggs = []\n",
    "    for a in client.list_aggs(ticker=symbol, multiplier=1, timespan=\"day\", from_=start_date, to=end_date, limit=50000):\n",
    "        aggs.append(a)\n",
    "    return aggs\n",
    "\n",
    "def get_bars_as_dataframe(aggs):\n",
    "    bars = pd.DataFrame([{\n",
    "        \"timestamp\": a.timestamp,\n",
    "        \"open\": a.open,\n",
    "        \"high\": a.high,\n",
    "        \"low\": a.low,\n",
    "        \"close\": a.close,\n",
    "        \"volume\": a.volume,\n",
    "        \"vwap\": a.vwap,\n",
    "        \"transactions\": a.transactions\n",
    "    } for a in aggs])\n",
    "\n",
    "    # Convert timestamp to datetime and set as index\n",
    "    bars[\"date\"] = pd.to_datetime(bars[\"timestamp\"], unit=\"ms\")\n",
    "    bars.set_index(\"date\", inplace=True)\n",
    "    bars.drop(columns=[\"timestamp\"], inplace=True)\n",
    "\n",
    "    return bars.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming Raw Stock Market Data\n",
    "\n",
    "Let's say $y(t)$ represents our sample stock market data, where it is a function of time $t$. We want to create $\\hat{y}(t)$, trained on the data from $y(t)$, that closely resembles the original function while being modeled using a Fourier Series. This will enable us to input any time $t$ into $\\hat{y}(t)$ so that we can forecast prices that exist outside of the current time domain.\n",
    "\n",
    "$$\n",
    "y(t) \\;\\rightarrow\\; \\text{sample stock market data}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{y}(t) =\n",
    "\\underbrace{a + bt}_{\\text{linear drift}}\n",
    "+\n",
    "\\underbrace{\\sum_{k=0}^{K-1}\\big(c_k\\cos(\\omega_k t) + d_k\\sin(\\omega_k t)\\big)}_{\\text{oscillation}}\n",
    "$$\n",
    "\n",
    "where $a$ is the slope intercept, $b$ is the slope, $c_k$ is the coefficient for the $\\cos(\\omega_k t)$ component, $d_k$ is the coefficient for the $\\sin(\\omega_k t)$ component, $\\omega_k$ is the angular frequency, and $K$ is the number of frequencies included in the model.\n",
    "\n",
    "Note that $\\hat{y}(t)$ has a linear drift component as well as an oscillation component. In the real world, stock prices can trend upwards or downwards, captured by the linear slope $b$; and at $t=0$, the stock price may start at a non-zero number, hence the reason we offset with the value $a$. We will remove this linear drift component from a piece of our data when calculating the dominant angular frequencies (an intermediate step expanded below), but it's important to recognize that linear drift is still a part of our final resulting function.\n",
    "\n",
    "In order to create $\\hat{y}(t)$, we can break it down into five steps, at a high-level:\n",
    "\n",
    "1. **Prepare time series** - initialize sample data based on closing price\n",
    "\n",
    "2. **Calculate dominant angular frequencies** - this basically tells us where the strongest sinuisoidal waves exist in our data\n",
    "\n",
    "3. **Build design matrix** - creating a table that tells us how each of these waves combine at each point in time\n",
    "\n",
    "4. **Ridge regression** - find the smoothest mix of waves that match the stock market data\n",
    "\n",
    "5. **Calculate modeled data** - create the final output model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prepare Time Series\n",
    "\n",
    "#### A. Initialize Sample Data on Closing Price\n",
    "\n",
    "We can use the closing price on each trading day to model our Fourier Series. By taking the logarithm of the closing price, we convert multiplicative changes into additive ones, which stabilizes variance and makes the changes comparable across different price levels. We will represent our sample data as $y(t)$, where it is a function of $t = 0, 1, 2, \\dots, N-1$, and $N$ is the number of samples.\n",
    "\n",
    "e.g.\n",
    "\n",
    "$$\n",
    "\\text{Stock A: } \\$5 \\rightarrow \\$10,\\ +5\\ \\text{change}\n",
    "$$\n",
    "$$\n",
    "\\text{Stock B: } \\$100 \\rightarrow \\$105,\\ +5\\ \\text{change}\n",
    "$$\n",
    "$$\n",
    "\\text{Stock A: } \\log(5) \\approx 0.6990 \\rightarrow \\log(10) = 1,\\ +0.3010\\ \\text{change}\n",
    "$$\n",
    "$$\n",
    "\\text{Stock B: } \\log(100) = 2 \\rightarrow \\log(105) \\approx 2.0212,\\ +0.0212\\ \\text{change}\n",
    "$$\n",
    "\n",
    "Even though Stock A and B both changed by +5, a move from \\$5→\\$10 (100% increase) is very different from a move \\$100→\\$105 (5% increase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_time_series(\n",
    "    bars: pd.DataFrame,\n",
    "    use_log: bool = True\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "    # Initilize the time series y and its time index t -> sample data: y(t)\n",
    "    y = bars['close'].astype(float).to_numpy()\n",
    "    y = np.log(y) if use_log else y\n",
    "    t = np.arange(len(y), dtype=float)\n",
    "    \n",
    "    return y, t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Calculate Dominant Angular Frequencies\n",
    "\n",
    "#### A. Detrend\n",
    "\n",
    "Let's say that our sample data $ y(t) $ has a linear drift. We want to remove the linear drift component before calculating the top angular frequencies, because the Fourier transform will interpret even a slight linear drift as a very slow wave. Let's create a new function called $ \\tilde{y}(t) $, so we don't modify the original sample data.\n",
    "\n",
    "$$\n",
    "y(t) \\approx a + bt\n",
    "$$\n",
    "\n",
    "where $ a $ is the slope intercept and $ b $ is the slope\n",
    "\n",
    "$$\n",
    "\\tilde{y}(t) = y(t) - (a + bt)\n",
    "$$\n",
    "\n",
    "$a + bt$ can also be represented as the matrix multiplication of $A$ and $\\beta$, where\n",
    "\n",
    "$$\n",
    "A =\n",
    "\\begin{bmatrix}\n",
    "1 & t_0 \\\\\n",
    "1 & t_1 \\\\\n",
    "1 & t_2 \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "1 & t_{N-1}\n",
    "\\end{bmatrix},\n",
    "\\qquad\n",
    "\\beta =\n",
    "\\begin{bmatrix}\n",
    "a \\\\\n",
    "b\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "a + bt = A\\beta\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde{y}(t) = y(t) - A\\beta\n",
    "$$\n",
    "\n",
    "We want to solve for the best $\\beta$ using the least squares loss function to find the best linear fit, which we will denote $\\beta^*$.\n",
    "\n",
    "$$\n",
    "\\text{find } \\beta \\text{ such that } y \\approx A\\beta \\rightarrow \\beta^*\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\beta^* = \\arg\\min_{\\beta} \\lVert y - A\\beta \\rVert^2\n",
    "$$\n",
    "\n",
    "$L(\\beta)$ is the least squares loss function. We're going to solve the first derivative and second derivative of $L$ to get $\\beta^*$.\n",
    "\n",
    "$$\n",
    "L(\\beta) = \\lVert y - A\\beta \\rVert^2\n",
    "$$\n",
    "\n",
    "Aside: $\\lVert y - A\\beta \\rVert = \\sqrt{(y - A\\beta)^2}$  \n",
    "therefore $\\lVert y - A\\beta \\rVert^2 = (y - A\\beta)^2$\n",
    "\n",
    "$$\n",
    "= (y - A\\beta)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "= (y - A\\beta)^T (y - A\\beta)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= y^Ty - y^TA\\beta - \\beta^TA^Ty + \\beta^TA^TA\\beta\n",
    "$$\n",
    "\n",
    "Aside: $y^TA\\beta = \\beta^TA^Ty$  \n",
    "therefore $-y^TA\\beta - \\beta^TA^Ty = -2\\,\\beta^TA^Ty$\n",
    "\n",
    "$$\n",
    "= y^Ty - 2\\,\\beta^TA^Ty + \\beta^TA^TA\\beta\n",
    "$$\n",
    "\n",
    "Aside: $\\frac{d}{d\\beta}(\\beta^T A^T A \\beta) = (A^T A + (A^T A)^T)\\beta = (A^T A + A^T A)\\beta = 2A^T A\\beta$\n",
    "\n",
    "$$\n",
    "\\frac{dL}{d\\beta} = 0 - 2A^Ty + 2A^TA\\beta\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d^2L}{d\\beta^2} = 0 + 0 + 2A^TA\n",
    "$$\n",
    "\n",
    "$\\frac{dL}{d\\beta} = 0$ indicates a unique global minimum when $\\frac{d^2L}{d\\beta^2}$ (aka the Hessian) is positive definite. We just need to prove that $A^TA$ is positive definite (ignoring the coefficient 2).\n",
    "\n",
    "A symmetric matrix like $A^TA$ is positive definite if:\n",
    "\n",
    "$$\n",
    "z^TA^TAz > 0 \\text{ for all } z \\neq 0\n",
    "$$\n",
    "\n",
    "where $z$ is a non-zero vector.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "z^TA^TAz \n",
    "&= (Az)^T(Az) \\\\\n",
    "&= (Az)^2 \\\\\n",
    "&= \\lVert Az \\rVert^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The two columns of $A$ are linearly independent - not multiples of each other.\n",
    "\n",
    "Since the columns of $A$ are linearly independent, then $Az = 0$ only has the trivial solution $z = 0$. For all non-zero $z$ vectors (where at least one component is non-zero), $Az \\neq 0$ and $\\lVert Az \\rVert^2 > 0$. \n",
    "\n",
    "Therefore, $A^TA$ is positive definite, and the same is true for any scalar multiple of $A^TA$, like $\\frac{d^2L}{d\\beta^2}$.\n",
    "\n",
    "Since $\\frac{d^2L}{d\\beta^2}$ is positive definite, $\\beta^*$ is the $\\beta$ where $\\frac{dL}{d\\beta} = 0$.\n",
    "\n",
    "$$\n",
    "0 = -2A^Ty + 2A^TA\\beta^*\n",
    "$$\n",
    "\n",
    "$$\n",
    "-2A^TA\\beta^* = -2A^Ty\n",
    "$$\n",
    "\n",
    "$$\n",
    "A^TA\\beta^* = A^Ty\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\beta^* = (A^TA)^{-1} A^Ty\n",
    "$$\n",
    "\n",
    "We can use numpy to solve for $\\beta^*$, i.e.\n",
    "\n",
    "```python\n",
    "beta = numpy.linalg.solve(ATA, ATy)\n",
    "```\n",
    "\n",
    "Just for fun, we can expand the matrix multiplication for $\\beta^*$, which sheds some light on what numpy is doing under the hood.\n",
    "\n",
    "Aside: for a $2\\times 2$ matrix, $\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}^{-1} = \\frac{1}{ad - bc}\\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}$\n",
    "\n",
    "$$\n",
    "(A^TA)^{-1}\n",
    "=\n",
    "\\frac{1}{\\,N\\sum_{i=0}^{N-1} t_i^2 - \\left(\\sum_{i=0}^{N-1} t_i\\right)^2\\,}\n",
    "\\begin{bmatrix}\n",
    "\\sum_{i=0}^{N-1} t_i^2 & -\\sum_{i=0}^{N-1} t_i \\\\\n",
    "-\\sum_{i=0}^{N-1} t_i & N\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "A^Ty\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\sum_{i=0}^{N-1} y_i \\\\\n",
    "\\sum_{i=0}^{N-1} t_i y_i\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\beta^*\n",
    "=\n",
    "(A^TA)^{-1}A^Ty\n",
    "=\n",
    "\\frac{1}{\\,N\\sum_{i=0}^{N-1} t_i^2 - \\left(\\sum_{i=0}^{N-1} t_i\\right)^2\\,}\n",
    "\\begin{bmatrix}\n",
    "\\sum_{i=0}^{N-1} t_i^2 & -\\sum_{i=0}^{N-1} t_i \\\\\n",
    "-\\sum_{i=0}^{N-1} t_i & N\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\sum_{i=0}^{N-1} y_i \\\\\n",
    "\\sum_{i=0}^{N-1} t_i y_i\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\beta^*\n",
    "=\n",
    "\\frac{1}{\\,N\\sum_{i=0}^{N-1} t_i^2 - \\left(\\sum_{i=0}^{N-1} t_i\\right)^2\\,}\n",
    "\\begin{bmatrix}\n",
    "\\left(\\sum_{i=0}^{N-1} t_i^2\\right)\\left(\\sum_{i=0}^{N-1} y_i\\right)\n",
    "-\n",
    "\\left(\\sum_{i=0}^{N-1} t_i\\right)\\left(\\sum_{i=0}^{N-1} t_i y_i\\right)\n",
    "\\\\\n",
    "-\\left(\\sum_{i=0}^{N-1} t_i\\right)\\left(\\sum_{i=0}^{N-1} y_i\\right)\n",
    "+\n",
    "N\\left(\\sum_{i=0}^{N-1} t_i y_i\\right)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### B. Hann Window\n",
    "\n",
    "A Hann window is a smoothing function applied to our sample data before taking its Fourier transform to reduce the sharp discontinuities at its boundaries. By softening these boundaries, we reduce spectral leakage and keep our frequency components more concentrated around their true values.\n",
    "\n",
    "$$\n",
    "h(n) = \\frac{1}{2}\\left(1 - \\cos\\left(\\frac{2\\pi n}{N-1}\\right)\\right)\n",
    "\\quad n = [0, N-1]\n",
    "$$\n",
    "\n",
    "$$\n",
    "h(0) = h(N-1) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde{y} \\;\\Rightarrow\\; h\\,\\tilde{y}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### C. Real Fast Fourier Transform (RFFT)\n",
    "\n",
    "We convert the time-domain signal $\\tilde{y}(t)$ into the frequency domain using the Discrete Fourier Transform (computed efficiently via the RFFT) to extract dominant angular frequencies $\\omega_k$.\n",
    "\n",
    "$$\n",
    "F(k) = \\sum_{n=0}^{N-1} \\tilde{y}(n) \\, e^{-j\\, 2\\pi \\frac{kn}{N}}\n",
    "$$\n",
    "\n",
    "where $F(k)$ is the complex DFT coefficient at frequency bin $k$, $N$ is the number of samples, and $j$ is the imaginary unit $\\sqrt{-1}$. \n",
    "\n",
    "Because $\\tilde{y}(t)$ is real-valued, the DFT satisfies the Hermitian symmetry $F(k) = \\overline{F(N-k)}$, meaning the spectrum is mirrored and only the first $\\left\\lfloor \\tfrac{N}{2} \\right\\rfloor + 1$ coefficients contain unique frequency information. \n",
    "\n",
    "The magnitude $|F(k)|$ represents the strength of the component at angular frequency $\\omega_k$.\n",
    "\n",
    "$$\n",
    "\\omega_k = 2\\pi \\frac{k}{N}.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{y}(t) =\n",
    "a + bt +\n",
    "\\sum_{k=0}^{K-1}\n",
    "\\big(c_k \\cos(\\omega_k t) + d_k \\sin(\\omega_k t)\\big)\n",
    "$$\n",
    "\n",
    "where $K = \\left\\lfloor \\tfrac{N}{2} \\right\\rfloor + 1$, the number of frequencies selected from the RFFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dominant_angular_frequencies(\n",
    "    y: np.ndarray,\n",
    "    t: np.ndarray,\n",
    "    use_detrend: bool = True,\n",
    "    use_hann: bool = True,\n",
    "    sample_spacing: float = 1.0,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "    # If sampled data is too small, return\n",
    "    N = y.size\n",
    "    if N < 2:\n",
    "        return np.array([], dtype=float), np.array([], dtype=float)\n",
    "\n",
    "    y_tilda = y.copy()\n",
    "\n",
    "    # (A) Detrend\n",
    "    if use_detrend:\n",
    "        A = np.column_stack([np.ones_like(t), t])\n",
    "        ATA = A.T @ A\n",
    "        ATy = A.T @ y\n",
    "        beta = np.linalg.solve(ATA, ATy)\n",
    "        y_tilda = y - (A @ beta)\n",
    "\n",
    "    # (B) Hann window\n",
    "    if use_hann:\n",
    "        y_tilda = y_tilda * np.hanning(N)\n",
    "\n",
    "    # (C) Fast Fourier Transform (FFT)\n",
    "    F = np.fft.rfft(y_tilda)                              \n",
    "    freqs = np.fft.rfftfreq(N, d=sample_spacing)     \n",
    "    magnitudes = np.abs(F)                                \n",
    "    omegas = 2.0 * np.pi * freqs\n",
    "    \n",
    "    return omegas, magnitudes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build Design Matrix\n",
    "\n",
    "We construct the design matrix $X$ as:\n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "1 & t_0 & \\cos(\\omega_0 t_0) & \\sin(\\omega_0 t_0) & \\cdots & \\cos(\\omega_{K-1} t_0) & \\sin(\\omega_{K-1} t_0) \\\\\n",
    "1 & t_1 & \\cos(\\omega_0 t_1) & \\sin(\\omega_0 t_1) & \\cdots & \\cos(\\omega_{K-1} t_1) & \\sin(\\omega_{K-1} t_1) \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "1 & t_{N-1} & \\cos(\\omega_0 t_{N-1}) & \\sin(\\omega_0 t_{N-1}) & \\cdots & \\cos(\\omega_{K-1} t_{N-1}) & \\sin(\\omega_{K-1} t_{N-1})\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Note that the first two columns correspond to the linear drift component in $\\hat{y}$, and the following columns correspond to the oscillation component. \n",
    "\n",
    "In the Ridge Regression step, we will use the design matrix $X$ to calculate $\\theta$ such that $\\hat{y} = X \\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_design_matrix(\n",
    "    t: np.ndarray,\n",
    "    omegas: np.ndarray,\n",
    "    include_linear_drift: bool = True\n",
    ") -> np.ndarray:\n",
    "    cols = []\n",
    "    \n",
    "    if include_linear_drift:\n",
    "        cols.append(np.ones_like(t, dtype=float))\n",
    "        cols.append(t.astype(float))\n",
    "        \n",
    "    for w in omegas:\n",
    "        cols.append(np.cos(w * t))\n",
    "        cols.append(np.sin(w * t))\n",
    "    \n",
    "    X = np.column_stack(cols).astype(float)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Ridge Regression\n",
    "\n",
    "We can write our model in matrix form as $\\hat{y} = X\\theta$, where\n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "1 & t_0 & \\cos(\\omega_0 t_0) & \\sin(\\omega_0 t_0) & \\cdots & \\cos(\\omega_{K-1} t_0) & \\sin(\\omega_{K-1} t_0) \\\\\n",
    "1 & t_1 & \\cos(\\omega_0 t_1) & \\sin(\\omega_0 t_1) & \\cdots & \\cos(\\omega_{K-1} t_1) & \\sin(\\omega_{K-1} t_1) \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "1 & t_{N-1} & \\cos(\\omega_0 t_{N-1}) & \\sin(\\omega_0 t_{N-1}) & \\cdots & \\cos(\\omega_{K-1} t_{N-1}) & \\sin(\\omega_{K-1} t_{N-1})\n",
    "\\end{bmatrix},\n",
    "\\qquad\n",
    "\\theta =\n",
    "\\begin{bmatrix}\n",
    "a \\\\\n",
    "b \\\\\n",
    "c_0 \\\\\n",
    "d_0 \\\\\n",
    "\\vdots \\\\\n",
    "c_{K-1} \\\\\n",
    "d_{K-1}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{y} =\n",
    "\\begin{bmatrix}\n",
    "\\hat{y}(t_0) \\\\\n",
    "\\hat{y}(t_1) \\\\\n",
    "\\vdots \\\\\n",
    "\\hat{y}(t_{N-1})\n",
    "\\end{bmatrix}\n",
    "=\n",
    "a + bt + \\sum_{k=0}^{K-1}\\big(c_k\\cos(\\omega_k t) + d_k\\sin(\\omega_k t)\\big)\n",
    "$$\n",
    "\n",
    "We want to solve for the best $\\theta$ using the least squares loss function with ridge regularization, which we will denote $\\theta^*$.\n",
    "\n",
    "$$\n",
    "\\text{find }\\theta\\text{ such that } y \\approx X\\theta \\;\\rightarrow\\; \\theta^*\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta^* = \\arg\\min_{\\theta}\\left(\\lVert y - X\\theta\\rVert^2 + \\alpha\\lVert\\theta\\rVert^2\\right)\n",
    "$$\n",
    "\n",
    "where $\\alpha \\geq 0$.\n",
    "\n",
    "Note that this process is very similar to how we solved for $\\beta^*$ for $y \\approx A\\beta$, except for one key difference — the ridge penalty $\\alpha\\lVert\\theta\\rVert^2$. We introduce $\\alpha\\lVert\\theta\\rVert^2$ to make the problem well-posed when the columns of $X$ are correlated.\n",
    "\n",
    "Recall that for a simple linear drift model,\n",
    "\n",
    "$$\n",
    "A =\n",
    "\\begin{bmatrix}\n",
    "1 & t_0 \\\\\n",
    "1 & t_1 \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "1 & t_{N-1}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "the two columns $1$ and $t$ are completely independent and will never be correlated.\n",
    "\n",
    "However, for the design matrix $X$, there is a possibility that some of the oscillation components could be correlated, which would cause the coefficients in $\\theta$ to become large and unstable.\n",
    "\n",
    "The ridge penalty $\\alpha\\lVert\\theta\\rVert^2$ shrinks these coefficients towards zero, especially important for the $c_k$ and $d_k$ terms, in order to make the fit more robust.\n",
    "\n",
    "$L(\\theta)$ is the least squares loss function. We're going to solve the first derivative and second derivative of $L$ to get $\\theta^*$\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\lVert y - X\\theta \\rVert^2 + \\alpha \\lVert \\theta \\rVert^2\n",
    "$$\n",
    "\n",
    "Aside: $\\lVert y - X\\theta \\rVert = \\sqrt{(y - X\\theta)^2}$  \n",
    "therefore $\\lVert y - X\\theta \\rVert^2 = (y - X\\theta)^2$\n",
    "\n",
    "$$\n",
    "= (y - X\\theta)^2 + \\alpha \\theta^2\n",
    "$$\n",
    "$$\n",
    "= (y - X\\theta)^T (y - X\\theta) + \\alpha \\theta^T \\theta\n",
    "$$\n",
    "$$\n",
    "= y^T y - y^T X \\theta - \\theta^T X^T y + \\theta^T X^T X \\theta + \\alpha \\theta^T \\theta\n",
    "$$\n",
    "\n",
    "Aside: $y^T X \\theta = \\theta^T X^T y$  \n",
    "therefore $-y^T X \\theta - \\theta^T X^T y = -2 \\theta^T X^T y$\n",
    "\n",
    "$$\n",
    "= y^T y - 2\\theta^T X^T y + \\theta^T X^T X \\theta + \\alpha \\theta^T \\theta\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{dL}{d\\theta} = 0 - 2X^T y + 2X^T X \\theta + 2\\alpha \\theta\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{d^2L}{d\\theta^2}\n",
    "&= 0 + 0 + 2X^T X + 2\\alpha I \\\\\n",
    "&= 2 (X^T X + \\alpha I)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$\\frac{dL}{d\\theta} = 0$ indicates a unique global minimum when $\\frac{d^2L}{d\\theta^2}$ is positive definite, and a (not necessarily unique) global minimum when positive semidefinite. We just need to prove that $X^T X + \\alpha I$ is positive definite or positive semidefinite (ignoring the coefficient $2$).\n",
    "\n",
    "A symmetric matrix like $X^TX + \\alpha I$ is positive definite if:\n",
    "\n",
    "$$\n",
    "z^T(X^TX + \\alpha I)z > 0 \\text{ for all } z \\neq 0\n",
    "$$\n",
    "\n",
    "and positive semidefinite if:\n",
    "\n",
    "$$\n",
    "z^T(X^TX + \\alpha I)z \\geq 0 \\text{ for all } z \\neq 0\n",
    "$$\n",
    "\n",
    "where $z$ is a non-zero vector and $\\alpha$ $\\geq$ 0.\n",
    "\n",
    "Since $\\alpha \\geq 0$, there are two cases to consider:\n",
    "\n",
    "*Case 1: $\\alpha = 0$*\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "z^T (X^T X + \\alpha I) z\n",
    "&= z^T X^T X z \\\\\n",
    "&= (Xz)^T (Xz) \\\\\n",
    "&= (Xz)^2 \\\\\n",
    "&= \\lVert Xz \\rVert^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Because the columns of $X$ may be linearly dependent, there exists some non-zero $z$ vector such that $Xz = 0$ and $\\lVert Xz \\rVert^2 = 0$.\n",
    "\n",
    "$$\n",
    "\\lVert Xz \\rVert^2 \\geq 0\n",
    "$$\n",
    "\n",
    "Therefore $X^T X + \\alpha I$, where $\\alpha = 0$, is positive semidefinite.\n",
    "\n",
    "*Case 2: $\\alpha > 0$*\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "z^T (X^T X + \\alpha I) z\n",
    "&= z^T X^T X z + \\alpha z^T z \\\\\n",
    "&= (Xz)^T (Xz) + \\alpha z^T z \\\\\n",
    "&= \\lVert Xz \\rVert^2 + \\alpha \\lVert z \\rVert^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We have proven that $\\lVert Xz \\rVert \\ge 0$ earlier, $\\lVert z \\rVert^2 > 0$ because $z$ is a non-zero vector, and $\\alpha > 0$.\n",
    "\n",
    "$$\n",
    "\\lVert Xz \\rVert^2 + \\alpha \\lVert z \\rVert^2 > 0\n",
    "$$\n",
    "\n",
    "Therefore, $X^T X + \\alpha I$, where $\\alpha > 0$, is positive definite.\n",
    "\n",
    "Since $\\frac{d^2L}{d\\theta^2}$ is positive semidefinite when $\\alpha = 0$ and positive definite when $\\alpha > 0$, $\\theta^*$ is the $\\theta$ where $\\frac{dL}{d\\theta} = 0$.\n",
    "\n",
    "$$\n",
    "\\frac{dL}{d\\theta} = -2 X^T y + 2 X^T X \\theta + 2 \\alpha \\theta\n",
    "$$\n",
    "\n",
    "$$\n",
    "0 = -2 X^T y + 2 X^T X \\theta^* + 2 \\alpha \\theta^*\n",
    "$$\n",
    "$$\n",
    "2 X^T y = 2 X^T X \\theta^* + 2 \\alpha \\theta^*\n",
    "$$\n",
    "$$\n",
    "X^T y = X^T X \\theta^* + \\alpha \\theta^*\n",
    "$$\n",
    "$$\n",
    "X^T y = (X^T X + \\alpha I) \\theta^*\n",
    "$$\n",
    "$$\n",
    "\\theta^* = (X^T X + \\alpha I)^{-1} X^T y\n",
    "$$\n",
    "\n",
    "We now have the ridge solution $\\theta^*$. However, this expression can become computationally unstable when $X$ is wide, which is the case if we add lots of correlated angular frequencies to build $X$.\n",
    "\n",
    "In order to make the solution even more robust, we can use the Singular Value Decomposition (SVD) of the design matrix, and substitute those terms into the ridge solution.\n",
    "\n",
    "$$\n",
    "X = U S V^T\n",
    "$$\n",
    "\n",
    "where $U$ is an orthonormal matrix describing the output directions in $X\\theta$-space ($\\hat{y}$ space), $S$ is a diagonal matrix of singular values describing how strongly each direction in $\\theta$-space is scaled by $X$, and $V$ is an orthonormal matrix describing the input directions in $\\theta$-space.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "X^T X\n",
    "&= (USV^T)^T (USV^T) \\\\\n",
    "&= V S U^T U S V^T \\\\\n",
    "&= V S I S V^T \\\\\n",
    "&= V S^2 V^T\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Aside: $U^T U = I$\n",
    "\n",
    "Aside: $S I S = S^2$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "X^T y\n",
    "&= (USV^T)^T y \\\\\n",
    "&= V S U^T y\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\theta^*\n",
    "&= (X^T X + \\alpha I)^{-1} X^T y \\\\\n",
    "&= (V S^2 V^T + \\alpha I)^{-1} V S U^T y \\\\\n",
    "&= V (S^2 + \\alpha I)^{-1} V^T V S U^T y \\\\\n",
    "&= V (S^2 + \\alpha I)^{-1} I S U^T y \\\\\n",
    "&= V (S^2 + \\alpha I)^{-1} S U^T y \\\\\n",
    "&= V \\left( \\frac{S}{S^2 + \\alpha I} \\right) U^T y\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Since $S$ is diagonal, let $s = [s_1, s_2, \\ldots, s_R]^T$ represent the diagonal entries of $S$, where $R$ is the rank of $X$ (the number of non-zero singular values).\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\theta^*\n",
    "&= V \\left( \\frac{s}{s^2 + \\alpha} (U^T y) \\right)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    alpha: float = 0.0,\n",
    ") -> np.ndarray:\n",
    "\n",
    "    # Calculate U, s, and V^T using Singular Value Decomposition\n",
    "    # where s is a vector representing the singular values of S\n",
    "    U, s, VT = np.linalg.svd(X, full_matrices=False)\n",
    "\n",
    "    # Calculate theta\n",
    "    theta = VT.T @ ((s / (s**2 + alpha)) * (U.T @ y))\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Calculate Modeled Data\n",
    "\n",
    "Now that we've solved for $X$ and $\\theta$, we can fully calculate $\\hat{y} = X\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_modeled_data(\n",
    "    X: np.ndarray,\n",
    "    theta: np.ndarray,\n",
    "    use_log: bool = True,\n",
    ") -> np.ndarray:\n",
    "\n",
    "    y_hat = X @ theta\n",
    "    return np.exp(y_hat) if use_log else y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       open     high       low   close       volume      vwap  \\\n",
      "date                                                                            \n",
      "2025-01-02 05:00:00  136.00  138.880  134.6300  138.31  198226166.0  137.1925   \n",
      "2025-01-03 05:00:00  140.01  144.900  139.7300  144.47  229300628.0  143.4879   \n",
      "2025-01-06 05:00:00  148.59  152.156  147.8201  149.43  265356909.0  150.2312   \n",
      "2025-01-07 05:00:00  153.03  153.130  140.0100  140.14  351724174.0  143.8269   \n",
      "2025-01-08 05:00:00  142.58  143.950  137.5600  140.11  227288848.0  140.6542   \n",
      "...                     ...      ...       ...     ...          ...       ...   \n",
      "2025-09-10 04:00:00  176.64  179.290  175.4700  177.33  226852020.0  177.4580   \n",
      "2025-09-11 04:00:00  179.68  180.280  176.4800  177.17  151159274.0  177.7323   \n",
      "2025-09-12 04:00:00  177.77  178.595  176.4500  177.82  124911026.0  177.7448   \n",
      "2025-09-15 04:00:00  175.67  178.850  174.5100  177.75  147061559.0  176.5506   \n",
      "2025-09-16 04:00:00  177.00  177.500  174.3800  174.88  140737775.0  175.5261   \n",
      "\n",
      "                     transactions  \n",
      "date                               \n",
      "2025-01-02 05:00:00       1566998  \n",
      "2025-01-03 05:00:00       1745661  \n",
      "2025-01-06 05:00:00       2139589  \n",
      "2025-01-07 05:00:00       3297977  \n",
      "2025-01-08 05:00:00       1983809  \n",
      "...                           ...  \n",
      "2025-09-10 04:00:00       2122717  \n",
      "2025-09-11 04:00:00       1408216  \n",
      "2025-09-12 04:00:00       1129215  \n",
      "2025-09-15 04:00:00       1608560  \n",
      "2025-09-16 04:00:00       1387121  \n",
      "\n",
      "[176 rows x 7 columns]\n",
      "[138.31 144.47 149.43 140.14 140.11 135.91 133.23 131.76 136.24 133.57\n",
      " 137.71 140.83 147.07 147.22 142.62 118.42 128.99 123.7  124.65 120.07\n",
      " 116.66 118.65 124.83 128.68 129.84 133.57 132.8  131.14 135.29 138.85\n",
      " 139.4  139.23 140.11 134.43 130.28 126.63 131.28 120.15 124.92 114.06\n",
      " 115.99 117.3  110.57 112.69 106.98 108.76 115.74 115.58 121.67 119.53\n",
      " 115.43 117.52 118.53 117.7  121.41 120.69 113.76 111.43 109.67 108.38\n",
      " 110.15 110.42 101.8   94.31  97.64  96.3  114.33 107.57 110.93 110.71\n",
      " 112.2  104.49 101.49  96.91  98.89 102.71 106.43 111.01 108.73 109.02\n",
      " 108.92 111.61 114.5  113.82 113.54 117.06 117.37 116.65 123.   129.93\n",
      " 135.34 134.83 135.4  135.57 134.38 131.8  132.83 131.29 135.5  134.81\n",
      " 139.19 135.13 137.38 141.22 141.92 139.99 141.72 142.63 143.96 142.83\n",
      " 145.   141.97 144.69 144.12 145.48 143.85 144.17 147.9  154.31 155.02\n",
      " 157.75 157.99 153.3  157.25 159.34 158.24 160.   162.88 164.1  164.92\n",
      " 164.07 170.7  171.37 173.   172.41 171.38 167.03 170.78 173.74 173.5\n",
      " 176.75 175.51 179.27 177.87 173.72 180.   178.26 179.42 180.77 182.7\n",
      " 182.06 183.16 181.59 182.02 180.45 182.01 175.64 175.4  174.98 177.99\n",
      " 179.81 181.77 181.6  180.17 174.18 170.78 170.62 171.66 167.02 168.31\n",
      " 170.76 177.33 177.17 177.82 177.75 174.88]\n"
     ]
    }
   ],
   "source": [
    "symbol = \"NVDA\"\n",
    "\n",
    "bars_start_date, bars_end_date = \"2025-01-01\", \"2025-09-30\"\n",
    "    \n",
    "aggs = list_daily_aggregates(symbol, bars_start_date, bars_end_date)\n",
    "bars = get_bars_as_dataframe(aggs)\n",
    "\n",
    "y, t = prepare_time_series(bars)\n",
    "omegas, magnitudes = calculate_dominant_angular_frequencies(y, t)\n",
    "X = build_design_matrix(t, omegas)\n",
    "theta = ridge_regression(X, y)\n",
    "y_hat = calculate_modeled_data(X, theta)\n",
    "\n",
    "print(bars[:-10])\n",
    "print(y_hat[:-10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
